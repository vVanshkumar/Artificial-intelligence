{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7790627a-817a-4fa3-88d7-dcdbce84a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face recogntion project\n",
    "import cv2\n",
    "\n",
    "# Load pre-trained classifiers for face and eye detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "def detect_faces_and_eyes(frame):\n",
    "    # Convert the image to grayscale for better detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around each face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Region of interest for detecting eyes\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "        \n",
    "        # Detect eyes within the face region\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "# Access the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Detect faces and eyes in the frame\n",
    "    processed_frame = detect_faces_and_eyes(frame)\n",
    "    \n",
    "    # Display the processed frame\n",
    "    cv2.imshow('Face and Eye Detection', processed_frame)\n",
    "    \n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ef6c64-961f-46e6-a4eb-54d819c2769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi! I'm an NLP chatbot. Type 'bye' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Chatbot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is tokenizer \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Bye! Take care.\n"
     ]
    }
   ],
   "source": [
    "# nlp project\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string  # For text preprocessing\n",
    "\n",
    "# Download NLTK data files\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # WordNet Dictionary\n",
    "\n",
    "# Sample text for the chatbot to learn from\n",
    "text = \"\"\"\n",
    "Hello! I am an NLP chatbot. I can help you with basic queries about natural language processing.\n",
    "NLP stands for Natural Language Processing, a field of AI focused on enabling machines to understand and respond to human language.\n",
    "It includes tasks like text analysis, sentiment analysis, and machine translation.\n",
    "Feel free to ask me anything about NLP or related topics.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize sentences and words\n",
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# Generate responses\n",
    "def response(user_response):\n",
    "    bot_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    \n",
    "    # Create TF-IDF model\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf[:-1])\n",
    "    idx = vals.argsort()[0][-1]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if req_tfidf == 0:\n",
    "        bot_response = \"I'm sorry, I don't understand you.\"\n",
    "    else:\n",
    "        bot_response = sent_tokens[idx]\n",
    "    \n",
    "    sent_tokens.pop()  # Remove user response to maintain consistency\n",
    "    return bot_response\n",
    "\n",
    "# Main chatbot loop\n",
    "def chatbot():\n",
    "    print(\"Chatbot: Hi! I'm an NLP chatbot. Type 'bye' to exit.\")\n",
    "    while True:\n",
    "        user_response = input(\"You: \").lower()\n",
    "        if user_response == 'bye':\n",
    "            print(\"Chatbot: Bye! Take care.\")\n",
    "            break\n",
    "        elif user_response in ['thanks', 'thank you']:\n",
    "            print(\"Chatbot: You're welcome!\")\n",
    "        else:\n",
    "            print(\"Chatbot:\", response(user_response))\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b64828-142b-4e29-828e-a6e2faa8817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string  # For text preprocessing\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('wordnet')  # WordNet Dictionary\n",
    "\n",
    "# Chatbot training text\n",
    "text = \"\"\"\n",
    "Hello! I am an NLP chatbot using NLTK. Ask me anything about NLP, AI, or related topics.\n",
    "Natural Language Processing, often abbreviated as NLP, is a field of artificial intelligence.\n",
    "It focuses on enabling machines to understand and process human language.\n",
    "Applications of NLP include chatbots, sentiment analysis, machine translation, and more.\n",
    "Feel free to ask about these topics, and I will do my best to assist you.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize text into sentences and words\n",
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a08c57-1ee2-4374-8fe5-e28db8a09750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the chatbot response\n",
    "def response(user_response):\n",
    "    bot_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    \n",
    "    # Generate TF-IDF vectors\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    \n",
    "    # Compute similarity\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf[:-1])\n",
    "    idx = vals.argsort()[0][-1]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if req_tfidf == 0:\n",
    "        bot_response = \"I'm sorry, I don't understand you.\"\n",
    "    else:\n",
    "        bot_response = sent_tokens[idx]\n",
    "    \n",
    "    sent_tokens.pop()  # Remove the user's input from the list\n",
    "    return bot_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680d30bd-1b0e-48ec-b726-6a25cafd52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot():\n",
    "    print(\"Chatbot: Hello! I'm an NLP chatbot. Type 'bye' to exit.\")\n",
    "    while True:\n",
    "        user_response = input(\"You: \").lower()\n",
    "        if user_response in ['bye', 'exit', 'quit']:\n",
    "            print(\"Chatbot: Goodbye! Take care.\")\n",
    "            break\n",
    "        elif user_response in ['thanks', 'thank you']:\n",
    "            print(\"Chatbot: You're welcome!\")\n",
    "        else:\n",
    "            print(\"Chatbot:\", response(user_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa006fb0-c381-4804-a28e-441957962de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! I'm an NLP chatbot. Type 'bye' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is your name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  'thanks', 'thank you']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Natural Language Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Natural Language Processing, often abbreviated as NLP, is a field of artificial intelligence.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  artificial intelligence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  AI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  sentiment analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I don't understand you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  NLP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I am an NLP chatbot using NLTK.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f9fa7-59b6-497a-a2e9-1e007af32f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
